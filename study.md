# 神经网络
神经网络运作的时候，上一层的激活值将决定下一层的激活值。
所以核心是一层的激活值是通过怎样的计算，算出下一层的激活值的。
最终输出层最亮的神经元对应的数字就是预测的结果。

# 手写数字识别
设计一个机制，可以把像素拼成短边，把短边拼成图案，把图案拼成数字。

给第2层的每一个神经元和第一层所有神经元间的每一条接线，都赋予一个权重值。权重只是数字。
拿第一层所有的激活值，乘以权重，然后加起来，得到第二层的激活值。

把权重值看做一个表格更好理解。把正的权重值标记成绿色，负的权重值标记红色。颜色越暗，表示它的权重越接近于０。
把关注区域的权重赋为正值，把其他所有的权重值一律赋为0.
这样一来，对所有的像素值取加权和，就只会累加我们关注区域的像素值。
如果想识别出这里是否存在一条边，只需要给周围一圈的像素赋予负的权重值。
这样当中间的像素亮，周围的像素暗时，加权和就能达到最大值。　？
这样计算出来的加权和可以是任意大小，但这个网络中，我们需要激活值都处在0与１之间,
所以我们就可以把加权和输入进某个函数，比如sigmoid函数，把它压缩到0与１之间。
所以这个神经元的激活值实际上就是一个对加权和到底有多正的打分。
但有时，即使加权和大于０时，我们也不想把神经元点亮。？
可能只有当和大于例如10的时候，才让它激发。
我们只要在加权和之后加上一个-10的数，再输入sigmoid函数。这个附加的数就叫做偏置。
权重告诉你这个第二层的神经元关注什么样的像素图案，偏置告诉你加权和得有多大，才能让神经元的激发变得有意义。
https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.999.0.0&vd_source=7cb227e7b8d0c454e45fcb3a7d9eb17a
12:22

# 损失函数
损失函数是衡量模型预测值与真实值差距的指标。
如果网络能对图像进行正确分类的时候,这个平方和就比较小。
https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.999.0.0&vd_source=7cb227e7b8d0c454e45fcb3a7d9eb17a
08：43
常见的损失函数有：
- 均方误差（MSE）：预测值与真实值之差的平方的平均值。
- 交叉熵（Cross-Entropy）：预测值与真实值之差的对数的平均值。
- 分类误差率（Classification Error Rate）：分类错误的样本数占所有样本数的比例。

# 机器学习工作原理
像是一道微积分习题，在找某个函数的最小值.
让网络学习实际就是让代价函数的值最小，为了达到这个效果，损失函数非常有必要是平滑的，
这样我们才能每次挪一点点，最后找到一个局部最小值，这也解释了人工神经元的激活的值是连续的。



3 06

# 斜率

# 梯度
函数的梯度指出了函数的最陡增长方向。
按梯度的方向走，函数值增长得就最快。
# 多元微积分

# 梯度下降法


#  




# 资料
![img.png](img.png)





https://www.bilibili.com/video/BV1ys411472E?p=10&vd_source=7cb227e7b8d0c454e45fcb3a7d9eb17a
3:56










